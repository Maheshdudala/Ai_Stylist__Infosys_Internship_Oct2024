# -*- coding: utf-8 -*-
"""AI Stylist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZWdwkIBySJIzOd5X3Kv9FPQhSEmmrwo
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/Final_Fashion_dataset.csv')
df.head()

df['id'] = df['id'].str.split('.').str[0]

df.head()

"""<h3>handling missing values</h3>"""

df.info()

df.id = df.id.astype('int64')

df.isnull().sum()

df.usage.value_counts()

df.dropna(inplace=True)

df.isnull().sum()

df.describe()

catnames = [ 'gender', 'masterCategory', 'subCategory', 'articleType',
       'baseColour', 'season','usage','Month']
numnames = ['id','year','ratings','Price (USD)']

"""<h3>categorical data analysis</h3>"""

for i in catnames:
    print(f'{i}: {df[i].unique()}')

# Filter out rows using boolean indexing
df = df[~df['masterCategory'].isin(["Personal Care", "Home", "Free Items"])]
df = df[~df['subCategory'].isin(["Perfumes", "Water Bottle"])]

for i in catnames:
    print(f'{i}: {df[i].unique()}')

df.shape

"""<h3>outlier detection</h3>"""

dict_unique = {
    "numerical_columns": numnames,
    "unique_values": [df[i].nunique() for i in numnames]
}

df_unique = pd.DataFrame(dict_unique)
df_unique

for i in numnames:
    plt.figure(figsize=(8,4))
    sns.boxplot(df[i])

# Scatter plot for Ratings vs Price (USD)
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='ratings', y='Price (USD)')
plt.title('Scatter Plot of Ratings vs Price (USD)')
plt.xlabel('Ratings')
plt.ylabel('Price (USD)')
plt.show()

q25,q75 = np.percentile(df["Price (USD)"],[25,75])

iqr = q75 - q25
iqr

q0 = q25 - 1.5*iqr
q100 = q75 + 1.4*iqr
q0,q100

df = df[(df['Price (USD)'] >= q0) & (df['Price (USD)'] <= q100)]

df

# Save the DataFrame as a CSV file
df.to_csv('df_cleaned.csv', index=False)

# Code to download the file in a Jupyter notebook or Colab
from google.colab import files
files.download('df_cleaned.csv')

for i in numnames:
    plt.figure(figsize=(8,4))
    sns.boxplot(df[i])

"""<h3>distribution analysis</h3>"""

plt.figure(figsize=(8, 6))
sns.histplot(df['Price (USD)'], bins=30, kde=True)
plt.title('Distribution of Prices (USD)')
plt.xlabel('Price (USD)')
plt.ylabel('Frequency')
plt.show()

# plotting histograms to check the distribution of numerical data
from scipy.stats import skew
for i in numnames:
    plt.figure(figsize=(8,4))
    print(f"{i}: ",df[i].skew())
    sns.histplot(df[i],kde=True)

"""<h3>Random oversampling for year column</h3>"""

from sklearn.utils import resample

# Separate the data into a list of dataframes by year
dfs = [df[df['year'] == year] for year in df['year'].unique()]

# Determine the target number of samples for each year (e.g., the average count)
target_samples = int(np.mean([len(sub_df) for sub_df in dfs]))

# Resample each year group to the target size
dfs_resampled = [resample(sub_df, replace=True, n_samples=target_samples, random_state=42) for sub_df in dfs]

# Combine all resampled groups into a single dataframe
df_balanced = pd.concat(dfs_resampled)

df_balanced.year.value_counts()

df_balanced.head()

sns.histplot(df_balanced["year"],kde=True)

"""<h3>categorical analysis</h3>"""

# understanding the distribution and frequency of categorical columns(demographics)
print('Frequency Distribution of categorical columns')
for i in catnames:
    if i == "articleType":
        continue
    plt.figure(figsize=(15, 4))
    ax = sns.countplot(data=df_balanced, x=i, palette='Set1')
    plt.xticks(rotation=90)
    for j in ax.containers:
        ax.bar_label(j)

"""<h3>bivariate analysis</h3>"""

# Scatter plot of Price vs Ratings
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_balanced, x='Price (USD)', y='ratings', hue='masterCategory', alpha=0.7)
plt.title('Scatter Plot of Price vs Ratings')
plt.xlabel('Price (USD)')
plt.ylabel('Ratings')
plt.legend(loc='upper left')
plt.show()

sns.boxplot(df_balanced,x='masterCategory',y='Price (USD)',palette='rocket')
plt.title("Box Plot of Master category by Price(USD)")

plt.figure(figsize=(12,4))
sns.boxplot(df_balanced,x='subCategory',y='ratings',palette='Set2')
plt.title("Box Plot of Ratings by SubCategory")
plt.xticks(rotation=90)
plt.show()

# Count plot of Master Category vs Ratings
plt.figure(figsize=(12, 6))
ax = sns.countplot(data=df_balanced, x='masterCategory', hue='ratings')
for i in ax.containers:
    plt.bar_label(i)
plt.title('Count Plot of Master Category vs Ratings')
plt.xticks(rotation=45)
plt.show()

# Master Category vs Gender
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df_balanced, x='masterCategory', hue='gender')
for i in ax.containers:
    plt.bar_label(i)
plt.title('Master Category Distribution by Gender')
plt.xticks(rotation=45)
plt.show()

df_balanced.subCategory.unique()

plt.figure(figsize=(12,6))
df_grouped = df_balanced.groupby(['subCategory', 'gender']).size().reset_index(name='count')
ax = sns.barplot(data=df_grouped, x='subCategory', y='count', hue='gender', order=df_balanced['subCategory'].value_counts().index[:10])
for i in ax.containers:
    plt.bar_label(i)
plt.xticks(rotation=90)
plt.show()

# Master Category vs Gender
plt.figure(figsize=(12,6))
df_grouped = df_balanced.groupby(['masterCategory', 'gender']).size().reset_index(name='count')
ax = sns.barplot(data=df_grouped, x='masterCategory', y='count', hue='gender')
for i in ax.containers:
    plt.bar_label(i)
plt.xticks(rotation=90)
plt.show()

# subcategory vs season
plt.figure(figsize=(12,6))
df_grouped = df_balanced.groupby(['subCategory', 'season']).size().reset_index(name='count')
ax = sns.barplot(data=df_grouped, x='subCategory', y='count', hue='season', order=df_balanced['subCategory'].value_counts().index[:10])
for i in ax.containers:
    plt.bar_label(i)
plt.xticks(rotation=90)
plt.show()

# base colour vs usage
plt.figure(figsize=(18,6))
df_grouped = df_balanced.groupby(['baseColour', 'usage']).size().reset_index(name='count')
ax = sns.barplot(data=df_grouped, x='baseColour', y='count', hue='usage', order=df_balanced['baseColour'].value_counts().index[:10])
for i in ax.containers:
    plt.bar_label(i)
plt.xticks(rotation=90)
plt.show()

gender_mapping = {'Men': 0, 'Women': 1}

# Replace the 'gender' column with numeric values
df_balanced['gender_numeric'] = df_balanced['gender'].map(gender_mapping)

plt.figure(figsize=(8,6))
# Calculate correlation on numeric columns only, including the new 'gender_numeric' column
sns.heatmap(df_balanced.select_dtypes(include=np.number).corr(), annot=True)
plt.show()

"""<h3>Time-based analysis/Tren analysis based on Month data</h3>"""

# Count plot for Month
plt.figure(figsize=(8, 4))
ax = sns.countplot(data=df_balanced, x='Month', order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'],
                   palette="Set2")
for i in ax.containers:
    plt.bar_label(i)
plt.title('Distribution of Purchases by Month')
plt.xticks(rotation=45)
plt.show()

# price(usd) by month
plt.figure(figsize=(10,6))
ax = sns.barplot(data=df_balanced,x="Month",y="Price (USD)",palette="Set1",
                 order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])
for i in ax.containers:
    plt.bar_label(i)
plt.xticks(rotation=90)
plt.show()

# Reset the index of your DataFrame to ensure it is unique:
df_balanced = df_balanced.reset_index(drop=True)

# Then, proceed with creating the pair plot:
sns.pairplot(df_balanced, diag_kind='kde', hue='masterCategory')
plt.suptitle('Pair Plot of Numerical Features', y=1.0)
plt.show()

"""# Advanced EDA

<h3>clustering analysis</h3>
"""

# label encoding categorical columns
from sklearn.preprocessing import LabelEncoder,StandardScaler
le = LabelEncoder()
for i in catnames:
    df_balanced[i] = le.fit_transform(df_balanced[i])

df_balanced

# scaling the columns
features_to_be_scaled = ['gender', 'masterCategory', 'subCategory', 'articleType',
    'baseColour', 'season', 'ratings', 'Price (USD)', 'Month', 'year','usage']
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_balanced[features_to_be_scaled])

df_scaled

# applying pca for dimanesionality reduction
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

# elbow method to decide clusters for k-means clustering analysis
from sklearn.cluster import KMeans
wcss = [] # within cluster sum of squares
for k in range(1,11):
    kmeans = KMeans(n_clusters=k,random_state=42)
    kmeans.fit(pca_result)
    wcss.append(kmeans.inertia_)

plt.plot(range(1,11),wcss,marker='o')
plt.xlabel("No. of clusters")
plt.ylabel("wcss")
plt.title("Elbow method for optimal k")
plt.grid(True)
plt.show()

df_pca = pd.DataFrame(pca_result,columns=['pca1','pca2'])
df_pca

kmeans = KMeans(n_clusters=4,random_state=42)
df_pca["cluster"] = kmeans.fit_predict(df_pca)
df_pca

df_balanced["cluster"] = df_pca['cluster']

# Visualize the clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_pca,
    x='pca1', y='pca2',
    hue='cluster',
    palette='viridis'
)
plt.title('Clustering of Products with PCA (Including Month and Year)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()

df_balanced.head()

df_balanced.shape

# Save the DataFrame as a CSV file
df_balanced.to_csv('Cleaned_Fashion_dataset.csv', index=False)

# Code to download the file in a Jupyter notebook or Colab
from google.colab import files
files.download('Cleaned_Fashion_dataset.csv')

import tensorflow as tf
import os

from google.colab import drive
drive.mount('/content/drive')

# Image processing parameters
IMAGE_SIZE = (60, 80)
BATCH_SIZE = 32

image_folder = '/content/drive/MyDrive/images'

# Create a list of full image paths from the CSV
image_paths = [os.path.join(image_folder, image_id) for image_id in df['filename']]

# Function to load and preprocess an image
def load_and_preprocess_image(path):
    img = tf.io.read_file(path)  # Read the image file
    img = tf.image.decode_jpeg(img, channels=3)  # Decode the JPEG image
    img = tf.image.resize(img, IMAGE_SIZE)  # Resize the image to target size
    img = img / 255.0  # Normalize pixel values to [0, 1]
    return img

# Create a TensorFlow dataset from image paths
dataset = tf.data.Dataset.from_tensor_slices(image_paths)  # Create dataset from image paths
dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)  # Load and preprocess images
dataset = dataset.batch(BATCH_SIZE)  # Batch the dataset
dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Optimize loading performance

for images_batch in dataset.take(1):
    print(f"Batch shape: {images_batch.shape}")

# Number of images
num_images = len(image_paths)

# Number of batches
num_batches = num_images // BATCH_SIZE + int(num_images % BATCH_SIZE != 0)  # Add 1 if there's a remainder
print(f"Total number of batches: {num_batches}")

for images_batch in dataset.take(5):
    plt.imshow(images_batch[0])
    plt.show()

